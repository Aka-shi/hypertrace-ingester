mainClass = org.hypertrace.core.viewcreator.pinot.PinotTableCreationTool

view.name = ServiceCallView
view.output.schema.class = org.hypertrace.viewgenerator.api.ServiceCallView
view.output.schema.url = "host:port/myView"

kafka.brokerAddress = "bootstrap:9092"
kafka.topicName = service-call-view-events
kafka.partitions = 1
kafka.replicationFactor = 1

pinot.controllerHost = pinot-controller
pinot.controllerPort = 9000
pinot.timeColumn = start_time_millis
pinot.timeUnit = MILLISECONDS
# todo: Add Attributes and Metrics after adding map type value
pinot.dimensionColumns = [tenant_id, end_time_millis, client_event_id, server_event_id, caller_service_id_str, caller_service, caller_api_id_str, caller_api, callee_service_id_str, callee_service, callee_api_id_str, callee_api, trace_id, transaction_name, request_url, request_method, protocol_name, response_status_code, callee_backend_id, callee_backend_name]
pinot.columnsMaxLength={}
pinot.metricColumns = [duration_millis, error_count, exception_count, num_calls]
pinot.invertedIndexColumns= []
pinot.tableName = serviceCallView
pinot.loadMode = MMAP
pinot.numReplicas = 1
pinot.retentionTimeValue = 5
pinot.retentionTimeUnit = DAYS
pinot.brokerTenant = defaultBroker
pinot.serverTenant = defaultServer
pinot.segmentAssignmentStrategy = BalanceNumSegmentAssignmentStrategy

pinot.streamConfigs =
{
    streamType: kafka,
    stream.kafka.consumer.type: LowLevel,
    stream.kafka.topic.name: service-call-view-events,
    stream.kafka.consumer.factory.class.name: "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
    stream.kafka.decoder.class.name: "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",
    stream.kafka.decoder.prop.schema.registry.rest.url: "http://schema-registry-service:8081",
    stream.kafka.hlc.zk.connect.string: "zookeeper:2181",
    stream.kafka.zk.broker.url: "zookeeper:2181",
    stream.kafka.broker.list: "bootstrap:9092",
    realtime.segment.flush.threshold.time: 3600000,
    realtime.segment.flush.threshold.size: 500000,
    stream.kafka.consumer.prop.auto.offset.reset: largest
}